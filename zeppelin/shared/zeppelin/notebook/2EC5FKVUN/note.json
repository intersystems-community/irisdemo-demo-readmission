{
  "paragraphs": [
    {
      "text": "%md\n# 1- Getting the data from InterSystems IRIS",
      "dateUpdated": "Apr 26, 2019 3:41:29 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e1- Getting the data from InterSystems IRIS\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293289962_-1712279334",
      "id": "20180918-171747_1379008521",
      "dateCreated": "Apr 26, 2019 3:41:29 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Introduction",
      "text": "%md \n\nThis notebook has been created with Scala as the standard programming language. \n\nBut we still need to use %spark magic to have a spark context automatically created for us.",
      "dateUpdated": "Apr 26, 2019 3:41:29 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThis notebook has been created with Scala as the standard programming language. \u003c/p\u003e\n\u003cp\u003eBut we still need to use %spark magic to have a spark context automatically created for us.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293289996_-446455453",
      "id": "20180913-200826_991580137",
      "dateCreated": "Apr 26, 2019 3:41:29 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Importing required libraries",
      "text": "%spark.pyspark\n\nfrom pyspark.sql import column, functions\n\nfrom pyspark.ml.linalg import SparseVector,VectorUDT\n\n# Transformers to prepare the features for training\nfrom pyspark.ml.feature import VectorAssembler, VectorIndexer\n\n# Algorithms (Estimators) to train our models (Transformers)\nfrom pyspark.ml.classification import RandomForestClassifier,DecisionTreeClassifier,GBTClassifier\n\n# To Evaluate the model against the test data\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Pipeline\nfrom pyspark.ml import Pipeline\n\nimport numpy as np\n\n# So we can export our final model to PMML\n\n#import com.intersystems.spark.ml._\n#import org.jpmml.sparkml._\n#import java.io.File\n",
      "user": "anonymous",
      "dateUpdated": "Jan 21, 2020 2:43:04 PM",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556293289997_-446840202",
      "id": "20180918-013102_236592080",
      "dateCreated": "Apr 26, 2019 3:41:29 PM",
      "dateStarted": "Jan 21, 2020 2:43:04 PM",
      "dateFinished": "Jan 21, 2020 2:43:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading the data from IRIS",
      "text": "%spark.pyspark\n\ndf \u003d spark.read.format(\"iris\").option(\"dbtable\",\"SELECT Comorbidities, CurrentComorbidities, DxAdmissionSource, DxAgeDischarged, DxAgeGroup, DxDischargeLocation, DxEncounterType, DxEndDate, DxEndDateFxDayMonthYear, DxGenderViaPatient, DxStartDate, DxStartDateFxDayMonthYear, MxAgeDischarged, MxAlcohol, MxDrugs, MxExSmoker, MxIsReAdmit, MxLOS, MxNeverSmoked, MxPatientIDViaPatient, MxSmoker, MxWillReAdmit FROM IRISDemo_Cube_MLEncounter.Fact\").load()\n\ndf.cache()\ndf.count()\n\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 1:21:19 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290001_-436067233",
      "id": "20180913-200917_773871114",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 1:21:19 PM",
      "dateFinished": "Apr 29, 2019 1:21:19 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Did we have any null fields?",
      "text": "%spark.pyspark\n\ndf.na.drop(\"any\").count()",
      "user": "anonymous",
      "dateUpdated": "Apr 26, 2019 7:35:46 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "23281\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290005_-437606228",
      "id": "20180914-175124_1973597318",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 26, 2019 7:35:46 PM",
      "dateFinished": "Apr 26, 2019 7:35:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSo, there are some rows with null cells. Let\u0027s take a look at the data.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSo, there are some rows with null cells. Let\u0026rsquo;s take a look at the data.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290009_-439145224",
      "id": "20180913-200944_722776012",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nz.show(df.select(\"Comorbidities\").distinct())",
      "user": "anonymous",
      "dateUpdated": "Apr 26, 2019 7:35:58 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 336.3,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290011_-438375726",
      "id": "20180917-214847_451933357",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 26, 2019 7:35:58 PM",
      "dateFinished": "Apr 26, 2019 7:36:02 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "%spark.pyspark\n\ndf.selectExpr(\"MxWillReAdmit\").distinct().show()\n\nredmissions \u003d df.filter(\"MxWillReAdmit\u003e0\").count()\n\npercReadmissions\u003dredmissions*100.0/df.count()\n\nprint(percReadmissions)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 1:21:34 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290015_-439914722",
      "id": "20180917-234026_719483720",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 1:21:34 PM",
      "dateFinished": "Apr 29, 2019 1:21:35 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# 3 - Training Helper Function",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e3 - Training Helper Function\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290022_-454919929",
      "id": "20180918-172036_1798997875",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Creating the Training Function",
      "text": "%spark.pyspark\n\n# Arguably, we might be cross-contaminating our training and testing pools if we simply sample encounters, since some of our data\n# involves patient histories. The sampleby string will allow us to sample by patient instead of by encounter, eliminating this possibility.\n\ndef runPipelineOnData(fulldata, pipeline, PMMLClassNamePrefix, sampleby \u003d \"\"):\n    \n    if sampleby \u003d\u003d \"\":\n        trainingData, testingData \u003d fulldata.randomSplit((0.7, 0.3), 0)\n    \n    else:\n        splits \u003d fulldata.select(sampleby).distinct().randomSplit((0.7, 0.3), 0)\n        trainingData \u003d fulldata.join(splits[0],on\u003dsampleby).drop(sampleby)\n        testingData \u003d fulldata.join(splits[1],on\u003dsampleby).drop(sampleby)\n    \n\n    pipelineModel \u003d pipeline.fit(trainingData)\n    evaluator \u003d BinaryClassificationEvaluator().setLabelCol(\"MxWillReAdmit\")\n    \n    trainpred \u003d pipelineModel.transform(trainingData)\n\n    train_auc \u003d evaluator.evaluate(trainpred)\n\n    predictions \u003d pipelineModel.transform(testingData)\n\n    test_auc \u003d evaluator.evaluate(predictions)\n\n    print(\"\\t\\tTrain AUC \u003d \" + str(train_auc))\n    print(\"\\t\\tTest AUC \u003d \" + str(test_auc))\n    \n    fileName\u003d\"/common_shared/pmml/IRISDemo.ImportedModel.\"+PMMLClassNamePrefix+\".pmml\"\n    \n    print(\"\\t\\tExporting the model to PMML file \" + fileName)\n    \n#    file \u003d new File(fileName)\n    \n#    val pmmlBuilder \u003d new org.jpmml.sparkml.PMMLBuilder(fulldata.schema, pipelineModel).buildFile(file)\n    \n    return pipelineModel\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:57:32 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556293290024_-457228422",
      "id": "20180918-204224_1942830640",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:57:32 PM",
      "dateFinished": "Apr 29, 2019 3:57:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# 4 - Preparing the data",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e4 - Preparing the data\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290026_-456458924",
      "id": "20180918-172147_839423680",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Translating comorbidities",
      "text": "%md\n\nThe comorbidities are in a usefully compact format in the cube (a comma-separated string of unsorted indices). However, this format isn\u0027t ideal for analysis, and PMML doesn\u0027t allow VectorAssemblers to use vector or array inputs, so we need to turn them into named columns. Also, the reason PMML doesn\u0027t allow that is because of the distinct possibility that somewhere in versioning or analysis of larger data sets, something will change which index is what, meaning it wants actual meaningful variable names. (There\u0027s no enforcement of that, but it\u0027s a good idea.) So, we take the indexing that IRIS has already done, and use it to create an already one-hot-encoded set of columns with actual column names.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe comorbidities are in a usefully compact format in the cube (a comma-separated string of unsorted indices). However, this format isn\u0026rsquo;t ideal for analysis, and PMML doesn\u0026rsquo;t allow VectorAssemblers to use vector or array inputs, so we need to turn them into named columns. Also, the reason PMML doesn\u0026rsquo;t allow that is because of the distinct possibility that somewhere in versioning or analysis of larger data sets, something will change which index is what, meaning it wants actual meaningful variable names. (There\u0026rsquo;s no enforcement of that, but it\u0026rsquo;s a good idea.) So, we take the indexing that IRIS has already done, and use it to create an already one-hot-encoded set of columns with actual column names.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290028_-458767418",
      "id": "20190416-181348_468845959",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# Given a column containing a vector to be unpacked, the dataframe it is in, an array of indices paired with human-relevant column names, and\n# a column name prefix for ensuring column name uniqueness, unpack that vector into columns.\ndef withColumns(incol, df, idx2col, col_prefix):\n    if len(idx2col)\u003d\u003d0:\n        return df\n    else:\n        colnum,colname \u003d idx2col[0]\n        return withColumns(incol,df.withColumn(col_prefix+\"_\"+colname,df[incol].getItem(colnum-1)),idx2col[1:],col_prefix)\n\ndef deepseeDimensionToOHECols(df, dimension, cls, url, user \u003d \"SuperUser\", password \u003d \"sys\", dropnull \u003d True, dropdimension \u003d True):\n    from pyspark.sql.types import ArrayType,DoubleType\n\n    # Start by reading the dimension (star) table from IRIS.\n    dimensionDF \u003d spark.read.format(\"jdbc\").option(\"driver\",\"com.intersystems.jdbc.IRISDriver\").option(\"url\",url).option(\"user\",user).option(\"password\",password).option(\"dbtable\",cls+\".\"+dimension).load()\n    \n    dimension_map \u003d dimensionDF.select(\"ID\", dimension).collect()\n    n_dimensions \u003d len(dimension_map)\n\n    # Here we take advantage of Spark\u0027s ability to turn an array of indices into a sparse vector. The reduction of the index by 1 is because Scala\n    # and Spark assume 0-indexing, while IRIS assumes 1-indexing.\n    dimArrToVec \u003d functions.udf(lambda xs: SparseVector(n_dimensions,\n        dict([(x-1,1.0) for x in xs])),\n        VectorUDT())\n    # And to turn a sparse vector into a dense array.\n    vecToArray \u003d functions.udf(lambda x: x.toArray().tolist(), ArrayType(DoubleType()))\n\n    dropcols \u003d [\"temp_column_deepseeDimensionToOHECols\"]\n    if (dropnull):\n        dropcols.append(dimension+\"_\u003cnull\u003e\")\n    if (dropdimension):\n        dropcols.append(dimension)\n    \n    return withColumns(incol \u003d \"temp_column_deepseeDimensionToOHECols\",\n    df \u003d df.withColumn(\"temp_column_deepseeDimensionToOHECols\" , vecToArray(dimArrToVec(functions.split(df[dimension], \",\").cast(\"array\u003cint\u003e\"))) ),\n    idx2col \u003d dimension_map,\n    col_prefix \u003d dimension+\"_\"\n).drop(*dropcols)\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:43:08 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556293290032_-447994448",
      "id": "20190416-162134_1700877540",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:43:08 PM",
      "dateFinished": "Apr 29, 2019 3:43:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# Derive useful numerical values from dates, and discard useless columns.\ndf_intermediate \u003d df.withColumn(\"EndMonth\", functions.month(df[\"DxEndDate\"])).withColumn(\"EndDay\", functions.dayofmonth(df[\"DxEndDate\"])).withColumn(\"StartMonth\", functions.month(df[\"DxStartDate\"])).withColumn(\"StartYear\", functions.year(df[\"DxStartDate\"])).withColumn(\"StartDay\", functions.dayofmonth(df[\"DxStartDate\"])).drop(\"DxEndDate\",\"DxEndDateFxDayMonthYear\",\"DxEndDateFxMonthYear\",\"DxEndDateFxYear\",\"DxStartDate\",\"DxStartDateFxDayMonthYear\",\"DxStartDateFxMonthYear\",\"DxStartDateFxYear\",\"ID\",\"%dspartition\",\"%sourceId\",\"DxAgeDischarged\")\n\n# Now use the function defined before to translate the comma-separated strings of indices into column arrays, and do so with both all comorbidities and the current comorbidities.\ndf2 \u003d deepseeDimensionToOHECols(deepseeDimensionToOHECols(df_intermediate,\"Comorbidities\",\"IRISDemo_Cube_MLEncounter\",\"jdbc:IRIS://readmission_RRLACESrv_1:51773/APPINT\"),\"CurrentComorbidities\",\"IRISDemo_Cube_MLEncounter\",\"jdbc:IRIS://readmission_RRLACESrv_1:51773/APPINT\").na.fill(0)\n\ndf2.cache()\n\ndf2.head(500)[499]\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:43:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290033_-448379197",
      "id": "20190416-171104_410553634",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:43:16 PM",
      "dateFinished": "Apr 29, 2019 3:43:31 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\ndf2.printSchema()\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:48:50 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- DxAdmissionSource: long (nullable \u003d true)\n |-- DxAgeGroup: long (nullable \u003d true)\n |-- DxDischargeLocation: long (nullable \u003d true)\n |-- DxEncounterType: long (nullable \u003d true)\n |-- DxGenderViaPatient: long (nullable \u003d true)\n |-- MxAgeDischarged: integer (nullable \u003d true)\n |-- MxAlcohol: double (nullable \u003d false)\n |-- MxDrugs: double (nullable \u003d false)\n |-- MxExSmoker: double (nullable \u003d false)\n |-- MxIsReAdmit: double (nullable \u003d false)\n |-- MxLOS: integer (nullable \u003d true)\n |-- MxNeverSmoked: double (nullable \u003d false)\n |-- MxPatientIDViaPatient: string (nullable \u003d true)\n |-- MxSmoker: double (nullable \u003d false)\n |-- MxWillReAdmit: double (nullable \u003d false)\n |-- EndMonth: integer (nullable \u003d true)\n |-- EndDay: integer (nullable \u003d true)\n |-- StartMonth: integer (nullable \u003d true)\n |-- StartYear: integer (nullable \u003d true)\n |-- StartDay: integer (nullable \u003d true)\n |-- Comorbidities__\u003cnull\u003e: double (nullable \u003d false)\n |-- Comorbidities__MYOCARDIAL INFARCTION: double (nullable \u003d false)\n |-- Comorbidities__CONGESTIVE HEART FAILURE: double (nullable \u003d false)\n |-- Comorbidities__CEREBROVASCULAR DISEASE: double (nullable \u003d false)\n |-- Comorbidities__PERIPHERY VASCULAR DISEASE: double (nullable \u003d false)\n |-- Comorbidities__DIABETES WITHOUT CHRONIC COMPLICATION: double (nullable \u003d false)\n |-- Comorbidities__CHRONIC PULMONARY DISEASE: double (nullable \u003d false)\n |-- Comorbidities__HEMIPLEGIA OR PARAPLEGIA: double (nullable \u003d false)\n |-- Comorbidities__MILD LIVER DISEASE: double (nullable \u003d false)\n |-- Comorbidities__METASTATIC SOLID TUMOR: double (nullable \u003d false)\n |-- Comorbidities__RENAL DISEASE: double (nullable \u003d false)\n |-- Comorbidities__MODERATE OR SEVERE LIVER DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__\u003cnull\u003e: double (nullable \u003d false)\n |-- CurrentComorbidities__MYOCARDIAL INFARCTION: double (nullable \u003d false)\n |-- CurrentComorbidities__CONGESTIVE HEART FAILURE: double (nullable \u003d false)\n |-- CurrentComorbidities__PERIPHERY VASCULAR DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__CHRONIC PULMONARY DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__METASTATIC SOLID TUMOR: double (nullable \u003d false)\n |-- CurrentComorbidities__RENAL DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__MODERATE OR SEVERE LIVER DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__MILD LIVER DISEASE: double (nullable \u003d false)\n |-- CurrentComorbidities__CEREBROVASCULAR DISEASE: double (nullable \u003d false)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290036_-449533444",
      "id": "20180918-034847_1303622092",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:48:50 PM",
      "dateFinished": "Apr 29, 2019 3:48:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSo, the only non-numeric item here is the Patient ID, which of course we won\u0027t be doing any analysis with.\nEverything else is already indexed thanks to the cube, and we\u0027ve filled in nulls.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSo, the only non-numeric item here is the Patient ID, which of course we won\u0026rsquo;t be doing any analysis with.\u003cbr/\u003eEverything else is already indexed thanks to the cube, and we\u0026rsquo;ve filled in nulls.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290038_-448763946",
      "id": "20190318-150503_1841446169",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\n# 5 - Basic Infrastructure",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e5 - Basic Infrastructure\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290039_-449148695",
      "id": "20180918-172407_2140342279",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Vector Assembler",
      "text": "%md\n\nWe need to transform our features into vectors before giving them to the ML algorithms. For that, we use a class called VectorAssembler(). It will take an array of column names and build a single column in our dataframe that will include all the selected features.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eWe need to transform our features into vectors before giving them to the ML algorithms. For that, we use a class called VectorAssembler(). It will take an array of column names and build a single column in our dataframe that will include all the selected features.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290039_-449148695",
      "id": "20180929-222517_176232123",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Vector Indexer",
      "text": "%md\n\nSpark needs to know if a feature is *categorical* or *numeric*. A categorical feature is something like Male/Female, Day of Week, etc. A numeric feature is (usually) a continuous value, though there are occasional cases where it makes sense to force an algorithm to look at something non-continuous as numeric.\n\nWe could define this by hand but, again, Spark can help us with that with a VectorIndexer. It will look each feature and figure out for us if it is a categorical feature or continuous feature. We must give it a hint, though. Parameter MaxCategories defines the maximum number of items a feature must have to be considered a category. There are thirty-one possible days of the month, and it makes more sense to treat them as categorical than continuous. We\u0027ll make sure MaxCategories is at least 32 (in case of a null, which would now be a 0). For the rest, we\u0027ll have to look.\n\nLet\u0027s see how many distinct values we have in our columns:",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSpark needs to know if a feature is \u003cem\u003ecategorical\u003c/em\u003e or \u003cem\u003enumeric\u003c/em\u003e. A categorical feature is something like Male/Female, Day of Week, etc. A numeric feature is (usually) a continuous value, though there are occasional cases where it makes sense to force an algorithm to look at something non-continuous as numeric.\u003c/p\u003e\n\u003cp\u003eWe could define this by hand but, again, Spark can help us with that with a VectorIndexer. It will look each feature and figure out for us if it is a categorical feature or continuous feature. We must give it a hint, though. Parameter MaxCategories defines the maximum number of items a feature must have to be considered a category. There are thirty-one possible days of the month, and it makes more sense to treat them as categorical than continuous. We\u0026rsquo;ll make sure MaxCategories is at least 32 (in case of a null, which would now be a 0). For the rest, we\u0026rsquo;ll have to look.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s see how many distinct values we have in our columns:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290041_-451457189",
      "id": "20180929-222723_1895839244",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nfor colname in df2.columns:\n    print(colname + \" has \" + str(df2.select(colname).distinct().count()) + \" distinct values \")\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:50:06 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290042_-450302942",
      "id": "20180929-223757_1630009029",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:50:06 PM",
      "dateFinished": "Apr 29, 2019 3:50:47 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "On training the intial pipeline",
      "text": "%md\n\nNote that the pipeline before we get to the actual model is pre-trained on the entire data set. This is to ensure that all categories can be dealt with in the final vector assembler/indexer combination, even if the training/testing split eliminates a category from the training set and it isn\u0027t used in the actual model.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eNote that the pipeline before we get to the actual model is pre-trained on the entire data set. This is to ensure that all categories can be dealt with in the final vector assembler/indexer combination, even if the training/testing split eliminates a category from the training set and it isn\u0026rsquo;t used in the actual model.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290046_-451841937",
      "id": "20190417-125525_1482958535",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Creating the basic infrastructure to be used when training our models",
      "text": "%spark.pyspark\n\n# Include as independent variables all non-discarded columns except Patient ID (which we will use to sample) and the dependent variable, whether the\n# encounter was followed by a readmission.\ncollist \u003d [col for col in df2.columns if col not in (\"MxPatientIDViaPatient\",\"MxWillReAdmit\")]\n\nvectorAssembler \u003d VectorAssembler().setInputCols(collist).setOutputCol(\"rawFeatures\")\n\nvectorIndexer \u003d VectorIndexer().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMaxCategories(35)\n\npipeintro \u003d Pipeline().setStages([vectorAssembler, vectorIndexer])\npipeIntro \u003d pipeintro.fit(df2)",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:52:46 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1556293290048_-466462396",
      "id": "20180929-222442_1424032271",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:52:46 PM",
      "dateFinished": "Apr 29, 2019 3:52:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\npipeIntro.transform(df2).show()",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:53:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290049_-466847145",
      "id": "20181009-155854_1101550713",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:53:01 PM",
      "dateFinished": "Apr 29, 2019 3:53:01 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# 6 - Training the Model\n",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e6 - Training the Model\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290061_-471464131",
      "id": "20180929-224804_1883794398",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# 6.1 - Binary Decision Tree\n",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003e6.1 - Binary Decision Tree\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290063_-470694634",
      "id": "20180918-151452_701599867",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6.1.1 - Training the Model",
      "text": "%spark.pyspark\n\n# The MaxBins parameter needs to be at least as large as the number of possible categories for any categorical variable. So, we set it to the same value as the VectorIndexer\u0027s MaxCategories parameter, which we\u0027d set earlier. MexDepth should be large enough to actually capture behavior, but not so large that it overfits the data. It\u0027s a tricky problem, but if you play with the parameters enough you can often get decent results.\ndecisionTreeClassifier \u003d DecisionTreeClassifier().setLabelCol(\"MxWillReAdmit\").setFeaturesCol(\"features\").setPredictionCol(\"prediction\").setMaxDepth(18).setMaxBins(vectorIndexer.getMaxCategories())\n\n# Put together the full pipeline, including the pre-fit prep portion.\npipelineDTC \u003d Pipeline().setStages([pipeIntro, decisionTreeClassifier])\n\n# And now fit the model, sampling training/testing sets by patient ID.\npipelineModelDTC \u003d runPipelineOnData(df2, pipelineDTC, \"DecisionTreeClassifier\", sampleby\u003d\"MxPatientIDViaPatient\")\n\ndecisionTreeClassificationModel \u003d pipelineModelDTC.stages[1]\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 3:58:45 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\t\tTrain AUC \u003d 0.859987253417\n\t\tTest AUC \u003d 0.682942547019\n\t\tExporting the model to PMML file /common_shared/pmml/IRISDemo.ImportedModel.DecisionTreeClassifier.pmml\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290066_-459536915",
      "id": "20180918-041035_1716649505",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 3:58:46 PM",
      "dateFinished": "Apr 29, 2019 3:58:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Let\u0027s see it!",
      "text": "%spark.pyspark\n\nprint(\"\\n\\nLearned classification tree model:\\n\" + decisionTreeClassificationModel.toDebugString)\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 4:00:09 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290067_-459921664",
      "id": "20180930-144333_333084227",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 4:00:09 PM",
      "dateFinished": "Apr 29, 2019 4:00:09 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Decision Tree Results",
      "text": "%md\n\nThis model\u0027s fit is not great, but not terrible. An AUC of roughly 0.6 in this case is probably worse than industry standard, though it\u0027s difficult to compare properly with the literature. It\u0027s more than a little overfit (test AUC is lower than train AUC by more than a few percent). Next, we\u0027ll try a random forest model, which should reduce overfitting, likely increasing the test AUC.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThis model\u0026rsquo;s fit is not great, but not terrible. An AUC of roughly 0.6 in this case is probably worse than industry standard, though it\u0026rsquo;s difficult to compare properly with the literature. It\u0026rsquo;s more than a little overfit (test AUC is lower than train AUC by more than a few percent). Next, we\u0026rsquo;ll try a random forest model, which should reduce overfitting, likely increasing the test AUC.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290095_-483006598",
      "id": "20190417-202522_1220516717",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 6.2 - Random Forest Classifier",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e6.2 - Random Forest Classifier\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290095_-483006598",
      "id": "20180918-171233_1149874704",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6.2.1 - Training the model",
      "text": "%spark.pyspark\n# Random forests should have smaller trees, but many of them. This is best done with a little manual exploration of the and then a parameter grid\nrandomForest \u003d RandomForestClassifier().setLabelCol(\"MxWillReAdmit\").setFeaturesCol(\"features\").setNumTrees(50).setMaxDepth(9).setMaxBins(vectorIndexer.getMaxCategories())\n\npipelineRF \u003d Pipeline().setStages([pipeIntro, randomForest])\n\nrandomForestPipelineModel \u003d runPipelineOnData(df2, pipelineRF, \"RandomForest\", sampleby\u003d\"MxPatientIDViaPatient\")\n\nrandomForestModel \u003d randomForestPipelineModel.stages[1]",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 4:01:49 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\t\tTrain AUC \u003d 0.860725218704\n\t\tTest AUC \u003d 0.813249817214\n\t\tExporting the model to PMML file /common_shared/pmml/IRISDemo.ImportedModel.RandomForest.pmml\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290096_-472618378",
      "id": "20180918-030614_825977513",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 4:01:49 PM",
      "dateFinished": "Apr 29, 2019 4:02:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6.2.2 - Let\u0027s see it!",
      "text": "%spark.pyspark\n\nprint(\"\\n\\nLearned random forest model:\\n\" + randomForestModel.toDebugString)\n",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 4:02:37 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290096_-472618378",
      "id": "20180930-144928_212907788",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 4:02:37 PM",
      "dateFinished": "Apr 29, 2019 4:02:37 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Random Forest Results",
      "text": "%md\n\nA massive increase in performance is welcome. An AUC of over 0.8 says this is probably among the better proposed models, though again it\u0027s difficult to compare properly with the literature. It\u0027s also much less overfit (test AUC is still lower than train AUC, but by a much smaller amount).",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eA massive increase in performance is welcome. An AUC of over 0.8 says this is probably among the better proposed models, though again it\u0026rsquo;s difficult to compare properly with the literature. It\u0026rsquo;s also much less overfit (test AUC is still lower than train AUC, but by a much smaller amount).\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290103_-473772625",
      "id": "20190417-203333_1638980127",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 6.3 - Boosted Trees",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e6.3 - Boosted Trees\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290104_-475696369",
      "id": "20190418-171945_110154800",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nboostedTrees \u003d GBTClassifier().setLabelCol(\"MxWillReAdmit\").setFeaturesCol(\"features\").setMaxIter(20).setMaxDepth(7).setMaxBins(35).setPredictionCol(\"rawPrediction\")\n\npipelineBT \u003d Pipeline().setStages([pipeIntro, boostedTrees])\n\nboostedTreesPipelineModel \u003d runPipelineOnData(df2, pipelineBT, \"BoostedTrees\", sampleby\u003d\"MxPatientIDViaPatient\")",
      "user": "anonymous",
      "dateUpdated": "Apr 29, 2019 4:03:48 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\t\tTrain AUC \u003d 0.793133326412\n\t\tTest AUC \u003d 0.73255378396\n\t\tExporting the model to PMML file /common_shared/pmml/IRISDemo.ImportedModel.BoostedTrees.pmml\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290105_-476081118",
      "id": "20190318-194839_2011656131",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "dateStarted": "Apr 29, 2019 4:03:48 PM",
      "dateFinished": "Apr 29, 2019 4:04:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Boosted Tree Results",
      "text": "%md\n\nBoosted trees aren\u0027t much better in this case than the standard decision tree. Random forest is clearly the best of the three tree-based options.",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eBoosted trees aren\u0026rsquo;t much better in this case than the standard decision tree. Random forest is clearly the best of the three tree-based options.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1556293290105_-476081118",
      "id": "20190418-172219_270231930",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "dateUpdated": "Apr 26, 2019 3:41:30 PM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1556293290106_-474926871",
      "id": "20180918-044125_1997992567",
      "dateCreated": "Apr 26, 2019 3:41:30 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "9 - Mostly-Complete But Outdated PySpark Trees Implementation",
  "id": "2EC5FKVUN",
  "angularObjects": {
    "2DHWQ7A7V:shared_process": [],
    "2DGTFGFBC:shared_process": [],
    "2DHATZD9M:shared_process": [],
    "2DKDARDPF:shared_process": [],
    "2DG8A819A:shared_process": [],
    "2DJVV28U3:shared_process": [],
    "2DKCAA3TS:shared_process": [],
    "2DHEPV9M3:shared_process": [],
    "2DKW8P766:shared_process": [],
    "2DGHX224C:shared_process": [],
    "2DGRSNE7G:shared_process": [],
    "2DGUG4SEP:shared_process": [],
    "2DHM86BYR:shared_process": [],
    "2DHYW1ZN6:shared_process": [],
    "2DJ16C4UE:shared_process": [],
    "2DJ4VH5DW:shared_process": [],
    "2DJBEDGYM:shared_process": [],
    "2DM19CFTF:shared_process": [],
    "2DHHCF91E:shared_process": [],
    "2DKCPR9P1:shared_process": []
  },
  "config": {},
  "info": {}
}